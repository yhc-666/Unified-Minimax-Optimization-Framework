{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Debiasing Methods on Semi-Synthetic Data\n",
    "\n",
    "This notebook evaluates MF_DR, MF_MRDR_JL, MF_DR_BIAS, and MF_DR_V2 methods on semi-synthetic data.\n",
    "\n",
    "Metrics evaluated:\n",
    "- Calibration Error (ECE)\n",
    "- Balancing Error (BMSE)\n",
    "- DR Bias\n",
    "- DR Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))), 'ICLR24-Kernel-Balancing'))\n",
    "\n",
    "# Import baseline methods\n",
    "from baselines import MF_DR, MF_MRDR_JL, MF_DR_BIAS, MF_DR_V2\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_and_preprocess_data(data_path: str = \"data/synthetic_data\") -> Tuple[np.ndarray, np.ndarray, int, int]:\n    \"\"\"\n    Load ground truth data and get dimensions, compute propensity scores.\n    \n    Returns:\n        ground_truth: 1D array of conversion labels\n        propensity: 1D array of propensity scores\n        num_users: number of users\n        num_items: number of items\n    \"\"\"\n    # Load ground truth\n    with open(data_path, \"rb\") as f:\n        ground_truth = pickle.load(f)\n    \n    # Load dimensions from predicted_matrix file\n    with open(\"data/predicted_matrix\", \"rb\") as f:\n        _ = pickle.load(f)  # predictions\n        num_users = pickle.load(f)\n        num_items = pickle.load(f)\n    \n    # Calculate propensity scores as in synthetic.py\n    propensity = np.copy(ground_truth)\n    p = 0.5\n    # Note: The original code has repeated assignments, resulting in:\n    # propensity[ground_truth == 1] = p^3 = 0.125\n    # propensity[ground_truth == 0] = p^4 = 0.0625\n    propensity[np.where(propensity == 1)] = p ** 3\n    propensity[np.where(propensity == 0)] = p ** 4\n    \n    print(f\"Loaded ground truth with shape: {ground_truth.shape}\")\n    print(f\"Number of users: {num_users}, Number of items: {num_items}\")\n    print(f\"Ground truth unique values: {np.unique(ground_truth)}\")\n    print(f\"Propensity for positive: {p**3:.4f}, for negative: {p**4:.4f}\")\n    \n    return ground_truth, propensity, num_users, num_items\n\n# Load data\nground_truth, propensity, num_users, num_items = load_and_preprocess_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pairs(ground_truth: np.ndarray, num_users: int, num_items: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert 1D ground truth array to (user_id, item_id) pairs with labels.\n",
    "    \n",
    "    Returns:\n",
    "        x_all: array of shape (n_samples, 2) with [user_id, item_id]\n",
    "        y_all: array of shape (n_samples,) with labels\n",
    "    \"\"\"\n",
    "    # Generate all (user, item) pairs\n",
    "    x_all = []\n",
    "    for user in range(num_users):\n",
    "        for item in range(num_items):\n",
    "            x_all.append([user, item])\n",
    "    \n",
    "    x_all = np.array(x_all)\n",
    "    y_all = ground_truth.flatten()\n",
    "    \n",
    "    return x_all, y_all\n",
    "\n",
    "# Convert to pairs\n",
    "x_all, y_all = convert_to_pairs(ground_truth, num_users, num_items)\n",
    "print(f\"Total number of (user, item) pairs: {len(x_all)}\")\n",
    "print(f\"Positive rate in ground truth: {y_all.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biased_observations(x_all: np.ndarray, y_all: np.ndarray, \n",
    "                                num_users: int, num_items: int,\n",
    "                                obs_ratio: float = 0.05,\n",
    "                                selection_bias: float = 0.7) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate biased observed data with selection bias.\n",
    "    \n",
    "    Args:\n",
    "        x_all: all (user, item) pairs\n",
    "        y_all: all labels\n",
    "        obs_ratio: ratio of data to observe\n",
    "        selection_bias: strength of selection bias (higher = more bias towards positive items)\n",
    "    \n",
    "    Returns:\n",
    "        x_obs: observed (user, item) pairs\n",
    "        y_obs: observed labels\n",
    "        obs_idx: indices of observed samples\n",
    "    \"\"\"\n",
    "    n_total = len(x_all)\n",
    "    \n",
    "    # Create observation probabilities with selection bias\n",
    "    # Items with y=1 have higher probability of being observed\n",
    "    base_prob = obs_ratio\n",
    "    obs_probs = np.ones(n_total) * base_prob\n",
    "    \n",
    "    # Increase observation probability for positive items\n",
    "    positive_idx = np.where(y_all == 1)[0]\n",
    "    obs_probs[positive_idx] = base_prob * (1 + selection_bias)\n",
    "    \n",
    "    # Normalize to ensure expected observation ratio\n",
    "    obs_probs = obs_probs * (obs_ratio * n_total) / obs_probs.sum()\n",
    "    obs_probs = np.clip(obs_probs, 0, 1)\n",
    "    \n",
    "    # Sample observations\n",
    "    obs_mask = np.random.binomial(1, obs_probs).astype(bool)\n",
    "    obs_idx = np.where(obs_mask)[0]\n",
    "    \n",
    "    x_obs = x_all[obs_idx]\n",
    "    y_obs = y_all[obs_idx]\n",
    "    \n",
    "    print(f\"Generated {len(x_obs)} biased observations ({len(x_obs)/n_total:.2%} of total)\")\n",
    "    print(f\"Positive rate in observations: {y_obs.mean():.4f} (vs {y_all.mean():.4f} in ground truth)\")\n",
    "    \n",
    "    return x_obs, y_obs, obs_idx\n",
    "\n",
    "# Generate biased observations\n",
    "x_obs, y_obs, obs_idx = generate_biased_observations(x_all, y_all, num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_splits(x_all: np.ndarray, y_all: np.ndarray, \n",
    "                           x_obs: np.ndarray, y_obs: np.ndarray,\n",
    "                           obs_idx: np.ndarray,\n",
    "                           test_ratio: float = 0.2,\n",
    "                           y_ips_ratio: float = 0.05) -> Dict:\n",
    "    \"\"\"\n",
    "    Create train/test splits and sample unbiased data (y_ips).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train/test data and y_ips\n",
    "    \"\"\"\n",
    "    # Split observed data into train/test\n",
    "    x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        x_obs, y_obs, obs_idx, test_size=test_ratio, random_state=42, stratify=y_obs\n",
    "    )\n",
    "    \n",
    "    # Sample y_ips (unbiased data) - 5% of all data, excluding test set\n",
    "    all_idx = np.arange(len(x_all))\n",
    "    available_idx = np.setdiff1d(all_idx, idx_test)  # Exclude test set\n",
    "    \n",
    "    n_ips = int(len(x_all) * y_ips_ratio)\n",
    "    ips_idx = np.random.choice(available_idx, size=n_ips, replace=False)\n",
    "    \n",
    "    x_ips = x_all[ips_idx]\n",
    "    y_ips = y_all[ips_idx]\n",
    "    \n",
    "    # Get true labels for test set (for evaluation)\n",
    "    y_test_true = y_all[idx_test]\n",
    "    \n",
    "    print(f\"\\nData splits:\")\n",
    "    print(f\"Train: {len(x_train)} samples, positive rate: {y_train.mean():.4f}\")\n",
    "    print(f\"Test: {len(x_test)} samples, positive rate: {y_test.mean():.4f}\")\n",
    "    print(f\"Y_ips (unbiased): {len(x_ips)} samples, positive rate: {y_ips.mean():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'x_train': x_train,\n",
    "        'y_train': y_train,\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test,\n",
    "        'y_test_true': y_test_true,\n",
    "        'x_ips': x_ips,\n",
    "        'y_ips': y_ips,\n",
    "        'x_all': x_all,\n",
    "        'y_all': y_all\n",
    "    }\n",
    "\n",
    "# Create train/test splits\n",
    "data_splits = create_train_test_splits(x_all, y_all, x_obs, y_obs, obs_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_calibration_error(y_true: np.ndarray, y_pred: np.ndarray, n_bins: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error (ECE).\n",
    "    \n",
    "    ECE measures the difference between predicted probabilities and actual outcomes.\n",
    "    Lower ECE indicates better calibration.\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Find predictions in this bin\n",
    "        in_bin = (y_pred > bin_lower) & (y_pred <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            # Calculate accuracy in this bin\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            # Average confidence in this bin\n",
    "            avg_confidence_in_bin = y_pred[in_bin].mean()\n",
    "            # Weighted absolute difference\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "# Test ECE\n",
    "test_y_true = np.array([0, 0, 1, 1])\n",
    "test_y_pred = np.array([0.1, 0.2, 0.8, 0.9])\n",
    "print(f\"Test ECE: {expected_calibration_error(test_y_true, test_y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_mse(x: np.ndarray, propensity_scores: np.ndarray, \n",
    "                  treated_idx: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Balancing Mean Squared Error (BMSE).\n",
    "    \n",
    "    BMSE measures how well the propensity scores balance the covariates.\n",
    "    Lower BMSE indicates better balancing.\n",
    "    \"\"\"\n",
    "    # Create treatment indicator\n",
    "    n = len(x)\n",
    "    treatment = np.zeros(n)\n",
    "    treatment[treated_idx] = 1\n",
    "    \n",
    "    # Calculate inverse propensity weights\n",
    "    weights = np.zeros(n)\n",
    "    weights[treatment == 1] = 1 / propensity_scores[treatment == 1]\n",
    "    weights[treatment == 0] = 1 / (1 - propensity_scores[treatment == 0])\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights[treatment == 1] /= weights[treatment == 1].sum()\n",
    "    weights[treatment == 0] /= weights[treatment == 0].sum()\n",
    "    \n",
    "    # Calculate weighted means for treated and control\n",
    "    x_treated = x[treatment == 1]\n",
    "    x_control = x[treatment == 0]\n",
    "    weights_treated = weights[treatment == 1]\n",
    "    weights_control = weights[treatment == 0]\n",
    "    \n",
    "    # Weighted mean difference\n",
    "    if len(x_treated) > 0 and len(x_control) > 0:\n",
    "        weighted_mean_treated = np.average(x_treated, weights=weights_treated, axis=0)\n",
    "        weighted_mean_control = np.average(x_control, weights=weights_control, axis=0)\n",
    "        bmse = np.mean((weighted_mean_treated - weighted_mean_control) ** 2)\n",
    "    else:\n",
    "        bmse = 0.0\n",
    "    \n",
    "    return bmse\n",
    "\n",
    "print(\"BMSE metric implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_robust_metrics(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                         y_obs: np.ndarray, propensity_scores: np.ndarray,\n",
    "                         n_bootstrap: int = 100) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate DR Bias and DR Variance using bootstrap.\n",
    "    \n",
    "    Returns:\n",
    "        dr_bias: Bias of the doubly robust estimator\n",
    "        dr_variance: Variance of the doubly robust estimator\n",
    "    \"\"\"\n",
    "    n = len(y_pred)\n",
    "    dr_estimates = []\n",
    "    \n",
    "    # True value (using all ground truth data)\n",
    "    true_value = y_true.mean()\n",
    "    \n",
    "    # Bootstrap to estimate bias and variance\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        idx = np.random.choice(n, size=n, replace=True)\n",
    "        \n",
    "        # DR estimator components\n",
    "        y_obs_b = y_obs[idx]\n",
    "        y_pred_b = y_pred[idx]\n",
    "        ps_b = propensity_scores[idx]\n",
    "        \n",
    "        # Clip propensity scores to avoid division by zero\n",
    "        ps_b = np.clip(ps_b, 0.01, 0.99)\n",
    "        \n",
    "        # DR estimator: E[Y] = E[Y_pred] + E[(Y_obs - Y_pred) / ps]\n",
    "        dr_estimate = np.mean(y_pred_b) + np.mean((y_obs_b - y_pred_b) / ps_b)\n",
    "        dr_estimates.append(dr_estimate)\n",
    "    \n",
    "    dr_estimates = np.array(dr_estimates)\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    dr_bias = np.abs(np.mean(dr_estimates) - true_value)\n",
    "    dr_variance = np.var(dr_estimates)\n",
    "    \n",
    "    return dr_bias, dr_variance\n",
    "\n",
    "print(\"DR metrics implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, data_splits: Dict, model_name: str) -> Dict[str, float]:\n    \"\"\"\n    Train model and evaluate all metrics.\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Evaluating {model_name}\")\n    print(f\"{'='*50}\")\n    \n    # Extract data\n    x_train = data_splits['x_train']\n    y_train = data_splits['y_train']\n    x_test = data_splits['x_test']\n    y_test = data_splits['y_test']\n    y_ips = data_splits['y_ips']\n    \n    # Train model\n    print(f\"Training {model_name}...\")\n    \n    # Model-specific training parameters\n    if model_name == \"MF_DR_V2\":\n        # MF_DR_V2 doesn't use y_ips parameter, it learns propensity internally\n        model.fit(x_train, y_train, \n                 num_epoch=100, lr=0.01, verbose=False)\n    else:\n        model.fit(x_train, y_train, y_ips=y_ips, \n                 num_epoch=100, batch_size=128, lr=0.01, verbose=False)\n    \n    # Make predictions on test set\n    y_pred = model.predict(x_test)\n    \n    # Apply sigmoid if needed (some models return logits)\n    if y_pred.min() < 0 or y_pred.max() > 1:\n        y_pred = 1 / (1 + np.exp(-y_pred))\n    \n    # Calculate ECE\n    ece = expected_calibration_error(y_test, y_pred)\n    \n    # Estimate propensity scores for BMSE calculation\n    # Using observed frequency as a simple propensity estimate\n    obs_rate = len(x_train) / (num_users * num_items)\n    propensity_scores = np.ones(len(x_test)) * obs_rate\n    \n    # Find which test samples were positive in training\n    positive_in_train = []\n    for i, (u, it) in enumerate(x_test):\n        train_match = (x_train[:, 0] == u) & (x_train[:, 1] == it)\n        if train_match.any() and y_train[train_match][0] == 1:\n            positive_in_train.append(i)\n    \n    # Calculate BMSE\n    bmse = balancing_mse(x_test, propensity_scores, np.array(positive_in_train))\n    \n    # Calculate DR Bias and Variance\n    dr_bias, dr_variance = doubly_robust_metrics(\n        y_test, y_pred, y_test, propensity_scores, n_bootstrap=50\n    )\n    \n    # Store results\n    results = {\n        'model': model_name,\n        'ECE': ece,\n        'BMSE': bmse,\n        'DR_Bias': dr_bias,\n        'DR_Variance': dr_variance\n    }\n    \n    print(f\"\\nResults for {model_name}:\")\n    print(f\"ECE: {ece:.4f}\")\n    print(f\"BMSE: {bmse:.4f}\")\n    print(f\"DR Bias: {dr_bias:.4f}\")\n    print(f\"DR Variance: {dr_variance:.4f}\")\n    \n    return results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Each Baseline Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "embedding_k = 64\n",
    "batch_size = 128\n",
    "\n",
    "models = {\n",
    "    'MF_DR': MF_DR(num_users=num_users, num_items=num_items, embedding_k=embedding_k),\n",
    "    'MF_MRDR_JL': MF_MRDR_JL(num_users=num_users, num_items=num_items, embedding_k=embedding_k),\n",
    "    'MF_DR_BIAS': MF_DR_BIAS(num_users=num_users, num_items=num_items, embedding_k=embedding_k),\n",
    "    'MF_DR_V2': MF_DR_V2(num_users=num_users, num_items=num_items, batch_size=batch_size, embedding_k=embedding_k)\n",
    "}\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for model in models.values():\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MF_DR\n",
    "results_mf_dr = evaluate_model(models['MF_DR'], data_splits, 'MF_DR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MF_MRDR_JL\n",
    "results_mf_mrdr = evaluate_model(models['MF_MRDR_JL'], data_splits, 'MF_MRDR_JL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MF_DR_BIAS\n",
    "results_mf_dr_bias = evaluate_model(models['MF_DR_BIAS'], data_splits, 'MF_DR_BIAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MF_DR_V2\n",
    "results_mf_dr_v2 = evaluate_model(models['MF_DR_V2'], data_splits, 'MF_DR_V2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [results_mf_dr, results_mf_mrdr, results_mf_dr_bias, results_mf_dr_v2]\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['ECE', 'BMSE', 'DR_Bias', 'DR_Variance']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    values = results_df[metric].values\n",
    "    models_names = results_df['model'].values\n",
    "    \n",
    "    bars = ax.bar(models_names, values, color=colors)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{value:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    ax.set_xticklabels(models_names, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'evaluation_metrics_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RANKING BY METRIC\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric in ['ECE', 'BMSE', 'DR_Bias', 'DR_Variance']:\n",
    "        # Lower is better for all metrics\n",
    "        sorted_df = results_df.sort_values(metric)\n",
    "        print(f\"\\n{metric} (lower is better):\")\n",
    "        for idx, row in sorted_df.iterrows():\n",
    "            print(f\"  {idx+1}. {row['model']}: {row[metric]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}